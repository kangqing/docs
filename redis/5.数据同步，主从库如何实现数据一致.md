# 5.数据同步，主从库如何实现数据一致

即使有了AOF和RDB这两种方式持久化数据，尽量保证少丢失数据，提升可靠性，但是也依然存在服务不可用的情况，比如说，只部署了一台Redis 实例，如果这个实例宕机了，在他的恢复期间，是无法服务新来的数据存取请求的。

我们经常说的 Redis 具有高可靠性，其实有两层含义：**一是数据尽量少丢失，二是服务尽量少中断，**AOF和RDB保证了前者，而对于后者，Redis的做法是增加副本冗余量，将一份数据同时保存在多个实例上。即使有一个实例出现了故障，需要一段时间才能恢复，其他的实例也能对外提供服务，不会影响业务。

多实例保存同一份数据，这就面临一个问题，这么多的副本，他们之间的数据时如何保持一致的呢？数据库的读写操作可以发给所有的实例吗？

实际上，Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。

- 读操作：主库、从库都可以接收；
- 写操作：首先到主库执行，然后主库将写操作同步给从库。

![img](https://yunqing-img.oss-cn-beijing.aliyuncs.com/hexo/article/202102/809d6707404731f7e493b832aa573a2f.jpg)

## 为什么采用读写分离的方式？

设想一下，上图中，不管是主库还是从库，都能接收客户端的写操作，那么，一个直接的问题就是：如果客户端对同一个数据修改了 3 次，然后分别请求发送到不同的实例上，那么这个数据在这三个副本的结果就不一致了，在读取的时候，就可能读到旧的数据。

如果非要保持这 3 个实例上的数据一致，就涉及到加锁、实例之间协调是否完成修改等一系列操作，带来很大的开销，这显然是不能接受的。

但是，主从库模式一旦采用了读写分离，所有数据的修改只会在主库上进行，不用协调三个实例，主库有了最新的数据后，会同步给从库，这样，主从库就能数据保持一致。

### 主从库之间如何进行第一次同步？

当多个实例的时候，他们之间就通过 `replicaof`命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。

例如，现在有实例1（172.16.19.3）和实例2（172.16.19.5）我们在实例2上执行以下命令之后，实例2就变成了实例1的从库，并从实例1上复制数据：

```bash
# 把当前实例作为 172.16.19.3 的从库
replicaof 172.16.19.3 6379
```

**主从库之前第一次数据同步的三个阶段：**

![img](https://yunqing-img.oss-cn-beijing.aliyuncs.com/hexo/article/202102/63d18fd41efc9635e7e9105ce1c33da1.jpg)

**第一阶段：**主从库建立连接协商同步，主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了。具体来说是，从库给主库发送`psync`命令，表示要践行数据同步，主库根据这个命令的参数来启动复制。`psync`命令包括了主库的`runID`和复制进度`offset`两个参数。

- runID: 是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标识这个实例。当从库和主库第一次复制时，因为不知道主库的 runID,所以将 runID设置为 `?`

- Offset: 此时设置为 -1，表示第一次复制。

主库收到`psync`命令后，会用`FULLRESYNC`响应命令带上两个参数：主库runID和主库的目前复制进度offset,返回给从库。从库收到响应后，会记录下这两个参数。

这里有个地方需要注意，`FULLRESYNC`响应命令表示第一次复制采用全量复制，也就是说，主库会把当前所有数据复制给从库。

**第二阶段**：主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的 RDB 文件。

具体来说，主库执行`bgsave`命令，生成 RDB 文件，紧接着将文件发送给从库。从库收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 `relicaof`命令开始和从库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空。

在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求，否则，Redis的服务就被中断了。但是这些请求中的写操作并没有记录到刚刚生成的 RDB 文件中。为了保证数据库主从一致性，主库会在内存中用专门的 `replication buffer`记录RDB文件生成后收到的所有写操作。

**第三阶段：**主库会把第二阶段执行过程中新收到的写命令，在发送给从库。具体的操作室，当主库完成 RDB 文件发送后，就会把此时 `replication buffer`中的修改操作发送给从库，从库在重新执行这些操作。这样，主从库就实现数据同步了。

## 主从级联模式分担全量复制时的主库压力

通过分析主从库第一次数据同步指导，一次全量复制中，对于主库来说，需要完成两个耗时的操作，生成 RDB 文件和传输 RDB 文件。

如何从库的数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于 fork 子进程生成 RDB 文件，进行数据全量同步。fork这个操作会阻塞主线程正常处理请求，从而导致主库响应应用程序请求速度变慢。此外，传输RDB 文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力。那么，有没有好的解决办法可以分担主库压力呢？

### 主 - 从 - 从 模式

刚才介绍的主从库模式中，所有的从库都是和主库连接，所有的全量复制也都是和主库进行的，现在我们可以通过**主-从-从模式将主库生成的RDB文件和传输RDB的压力，以级联的方式分散到从库上**，简单来说，我们在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库）用于级联其他从库。然后，我们可以在选择一些从库（例如三分之一的从库），在这些从库上执行如下命令，让他们和刚才所选的从库，建立起主从关系。

```bash
replicaof 高性能的从库IP 6379
```

这样一来，这些从库就会知道，在进行同步时，不用再和主库进行交互了，只要和级联的从库进行写操作同步就行了，这就可以减轻主库上的压力：

![img](https://yunqing-img.oss-cn-beijing.aliyuncs.com/hexo/article/202102/403c2ab725dca8d44439f8994959af45.jpg)

到这里，我们就了解了主从库之间全量复制实现同步数据的过程，以及使用主-从-从模式分担主库压力的方式。那么，一旦完成了全量复制，他们之间就会一直维护一个网络连接，主库会通过这个连接将后续陆续收到的命令操作在同步给从库，这个过程也称为`基于长连接的命令传播`，可以避免频繁建立连接的开销。

但是这个过程中存在风险点，最常见的就是网络断连或阻塞，如果网络断连，主从库之间就无法进行命令传播了，从库的数据自然也就没办法和主库保持一致了，客户端就可能在从库读取到旧数据。

## 主从库间网络断了怎么办

Redis 2.8之前，如果网络断了会重新进行一次全量复制，开销很大。

Redis 2.8之后，网络断了之后，主从库会采用增量复制的方式继续同步。增量复制是指只会把主从库网络断连这期间收到的命令同步给从库。增量复制时，主从库之间怎么保持同步的？这个奥妙在于`repl_backlog_buffer`这个缓冲区。我们看下他是如何用于增量命令的同步的。

当主从库断连后，主库会把断连期间收到的写操作命令，写入 `replication buffer`同时也会把这些操作命令写入`repl_backlog_buffer`这个缓冲区。

`repl_backlog_buffer`是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置。

刚开始的时候，主库和从库的写读位置在一起，这算是他们的起始位置，随着主库不断接受新的写操作，他在缓冲区中的写位置会逐步偏离开始位置，我们用偏移量衡量这个偏离距离的大小，对主来说，对应的偏移量就是`master_repl_offset`主库接受的新的写操作越多，这个值就会越大。

同样，从库复制完写操作命令后，他在缓冲区中的读位置也开始逐步偏移刚才的开始位置，此时，从库已复制的偏移量`slave_repl_offset`也在不断增加，正常情况下，这两个偏移量基本相等。

主从库之间连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的 `slave_repl_offset`发送给主库，主库会判断自己的`master_repl_offset`与其之间的差距。主库只用把`master_repl_offset`和`slave_repl_offset`之间的命令操作同步给从库就行了。如上图第二个环形，主从库之间只差了`put d e 和 put d f`两个命令，在增量复制时，主库只需要把他们同步给从库就行了。

![img](https://yunqing-img.oss-cn-beijing.aliyuncs.com/hexo/article/202102/20e233bd30c3dacb0221yy0c77780b16.jpg)

**注意：**因为`repl_backlog_buffer`是一个环形缓冲区，所以在缓冲区写满之后，主库会继续写入，此时，就会覆盖掉之前的写入，如果从库的读取速度比较慢，就可能会导致从库还未读取的操作被主库的新的写操作覆盖了，进而导致主从库间的数据不一致。

所以要避免这种情况，一般来说，可以调整`repl_backlog_size`这个参数。这个参数和所需缓冲空间大小有关。缓冲空间计算公式：缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际中，考虑到可能存在一些突发的请求压力，通常把这个缓冲空间扩大一倍，即 `repl_backlog_size` = 缓冲空间大小 * 2

举个例子：如果主库每秒写入 2000 个操作，每个操作大小是 2kb, 网络每秒传输 1000个操作，那么有1000 个操作需要缓冲起来，这就至少需要 2MB 的缓冲空间，否则，新的写入命令就会覆盖掉旧的操作，为了应对时机可能的突发压力，我们最终把 `repl_backlog_size`设置为 4MB。

当然如果并发请求量非常大，连续两倍的缓冲空间不足以存储的话，你可以设置成缓冲空间的 4 倍大小，另一方面，可以考虑使用切片集群来分担单个主库的请求压力。